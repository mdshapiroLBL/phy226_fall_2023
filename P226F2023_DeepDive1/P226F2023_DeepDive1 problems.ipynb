{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Physcs 226 Fall 2023 DeepDive1: Likelihood Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff4a2b2",
   "metadata": {},
   "source": [
    "The likelihood ${\\cal L}(x;\\theta)$ is the probability that a measurement of $x$ will yield a specific value of $\\theta$ for a given theory.  To determine the likelihood, you must know both the theory and the values of any parameters the theory depends on. If we have an ensemble of measurements, the overall likelihood is obtained from product of the likelihoods for the individual measurements:\n",
    "\n",
    "$$\n",
    "{\\cal L}(x;\\theta) = \\prod_{i=1}^n {\\cal L}_i (x_i;\\theta)\n",
    "$$\n",
    "\n",
    "Here $\\theta$ can represent one or more parameters. Likelihood functions can be used to estimate thesw parameters. To do this, we want to maximize the likelihood. Since products are a pain to deal with conputationally, usually it is more convenient to maximize the log of the likehood.\n",
    "\n",
    "$$\n",
    "\\begin{array}{rcl}\n",
    "\\frac{\\partial \\ln {\\cal L}}{\\partial  \\theta} &= &\\frac{\\partial} {\\partial \\theta} \\ln \\prod_{i=1}^n {\\cal L}_i   \\\\\n",
    "& = & \\frac{\\partial}{\\partial \\theta} \\sum_{i=1}^n \\ln {\\cal L}_i \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "We can do this because the logarithm is monotonically increasing on $[0, 1]$, so maximizing $\\cal L$ and maximizing its logarithm will yield the same set of parameters. The maximum of the likelihood function corresponds to the value of $\\theta$  that minimizes $- \\sum_i\\ln {\\cal L}_i $. If there ae several $\\theta$ parameters, we can minimize with respect to each one.\n",
    "\n",
    "As an example, suppose we have a process where the number of events is described by a Poisson distribution with mean number $\\mu$ and we measure this process with $N$ independent trials with measurements $n_i$ (where $n_i$ are integers). The likelihood for a single trial follows the Poisson distribution\n",
    "\n",
    "$$                                                                              \n",
    "{\\cal L}(n_i;\\mu) = \\frac{e^{-\\mu}(\\mu)^n}{n!}                              \n",
    "$$\n",
    "\n",
    "and the overall likehood for all the trials is the product of the $N$ likelhoods.  Then:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\cal L(\\vec{n};\\mu) & = \\prod_{i=1}^N  \\frac{ e^{-\\mu} (\\mu)^{n_i}}{n_i!}\\\\\n",
    "\\ln {\\cal L} & = \\sum_i\n",
    "\\left (  -\\mu + n_i\\ln \\mu\n",
    "-\\ln (n_i!)\n",
    "\\right )  \\\\\n",
    "   & = -N\\mu + \\left ( \\sum_i n_i \\right )\\ln \\mu + constant \\\\\n",
    "\\frac{\\partial {\\ln \\cal L}}{d\\mu }|_{\\hat\\mu=\\mu} & = -N + \\frac{\n",
    "\\sum_i n_i}{\\mu} = 0 \\\\\n",
    "\\hat \\mu & = \\frac{1}{N} \\sum_{i=1}^N n_i \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "As expected, the best estimator is the mean value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd19d40b",
   "metadata": {},
   "source": [
    "##  Question 1:  LogLikelihood and $\\chi^2$\n",
    "\n",
    "### Learning objectives\n",
    "In this question you will:\n",
    "\n",
    "- Relate the log of the likelihood function for a Gaussian pdf to the $\\chi^2$ distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15bc6970",
   "metadata": {},
   "source": [
    "We say in Homework \\#3 that the distribution of the sum (or average) of a large number of independent, identically distributed measurements will be approximately normal, regardless of the underlying distribution.  That means in the limit of high statistics, (for example a histogram where the number of events in each bin is large), the probability of seeing $n_i$ events in bin $i$ will be Gaussianly distributed:\n",
    "\n",
    " $$                                                                            \n",
    " G(x|\\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi}\\sigma} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} \n",
    " $$\n",
    "\n",
    "### 1a.\n",
    "\n",
    "Consider the case where we make $N$ independent measurements $x_i$.for a quantity $x$ that is Gaussianly distributed.\n",
    "Take the derivative the the log likelihood function with respect to $\\mu$ and show that the log likehood is minimized when the estimator $\\hat \\mu $ is the mean value of $x$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cfc5c3",
   "metadata": {},
   "source": [
    "Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75d8eb7",
   "metadata": {},
   "source": [
    "### 1b."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f944a2",
   "metadata": {},
   "source": [
    "Show that for the case of a Gaussian likehood function\n",
    "\n",
    "$$\n",
    "- 2 \\ln {\\cal L} = \\chi^2 + constant\n",
    "$$\n",
    "\n",
    "where $\\chi^2$ has the standard definition:\n",
    "\n",
    "$$\n",
    "\\chi^2 = \\sum_{i=1}^N \\frac{(x_i-\\mu)^2}{\\sigma^2}   \n",
    "$$\n",
    "\n",
    "You may find that there is a term in the \"constant\" that depends on $\\mu$ and/or $\\sigma$ from the normalization of the Gaussian distribution. This is to be expected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca41c717",
   "metadata": {},
   "source": [
    "Write your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Question 2: Likelihood functions for a decay model with background and maximum time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "### Learning objectives\n",
    "In this question you will:\n",
    "\n",
    "- Learn about log-likelihood fits using an example probability density function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "Consider the problem of determining the lifetime of a species of particle that we can stop in our detector by observing it decays in the presence of a constant background. The rate is given by\n",
    "$$\n",
    "R(t) = B + Se^{-t/\\tau}\n",
    "$$\n",
    "We'll take as the true parameters $B=1/$sec, $S=2/$sec, and $\\tau= 0.5$ sec. Each time we stop a particle in our detector, we will only wait to see if it decays for a maximum time $t_{max}=3$ seconds. We can imagine repeating the experiment many times (each experiment taking three seconds) to accumulate a lot of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "### 2a. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "First, let's generate some fake data. We will generate this data using the Monte Carlo technique known  as the *acceptance-rejection method* or *rejection sampling.* Suppose you want to generate events whose distribution follows the function $f(x)$ (here $f(x)$ is the probability distribution function). This can be achieved by generating points $(x_i,y_i)$ randomly in a two-dimensional space and keeping only the subset of the events where $y_i \\le f(x_i)$.  A more detailed explaination of the method can be found [here](https://en.wikipedia.org/wiki/Rejection_sampling).\n",
    "\n",
    "We will generate fake data in the following way.  Pick an $x_i$, the time $t$ of the decay, randomly  between 0 and $t_{max}$, and pick $y_i$ randomly between 0 and $R_{max}$, where $R_{max}$ is the maximum value of $R(t)$. Use $R(t)$  to you decide whether to keep this event. Do this until you've generated $N = 1000$ events which are accepted. What percentage of the time do you expect to keep an event? Validate that the acceptance rate of your data is close to the predicted amount."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "### 2b. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {
    "flags": [
     "problems",
     "solutions"
    ]
   },
   "source": [
    "Calculate the (negative) log-likelihood function, $- 2\\ln {\\cal L}$, for your data taking care to use a PDF normalized so that\n",
    "\n",
    "$$\n",
    "\\int_0^{t_{max}}f(t)dt=1\n",
    "$$\n",
    "\n",
    "Using this condition, express your likelihood in terms of two  free parameters, $\\tau$ and $\\kappa=S/B$. $\\kappa$ will be what we call the signal-to-background ratio for this problem.  (Note: you can see an example of how this works in the notebook from Section on September 7 2023).\n",
    "\n",
    "We will study the simulated data, pretending that we dont know what  values of $S$, $B$ and $\\tau$ were used to generate it.  We want to determine $\\tau$ and $\\kappa$ from the data. \n",
    "\n",
    "Write code to calculate the negative log-likelihood:\n",
    "\n",
    "$$\n",
    "- 2 \\ln {\\cal L} = - 2 \\sum_i \\ln f(\\kappa, \\tau, t_i)\n",
    "$$\n",
    "\n",
    "where the $t_i$ are the time values you accepted in the Monte Carlo process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5410e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3723a1d2",
   "metadata": {},
   "source": [
    "### 2c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce49ffee",
   "metadata": {},
   "source": [
    "There are lots of algorithms for finding the minimum of a non-linear function such as our negative log-likelihood, but we won't bother to use any of these algorithms here. Instead, we will explore the minimum by inspecting the behavour of the function. Plot (or just show the values in a 2-dimensional grid) the value of $- 2 \\ln \\mathcal{L}$ you obtain from your simulated data as you vary $\\kappa$  and $\\tau$ in the region of the true answer ($\\kappa=2$, $\\tau=0.5$). How close is the $\\tau$ that gives minimum negative log-likelihood to the true value of $\\Gamma$? Perform this test with a sample of $N=1000$ signal plus background events and again with $N=10000$ signal plus background events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f071e7b2",
   "metadata": {},
   "source": [
    "### 2d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f34d26",
   "metadata": {},
   "source": [
    "You proved in problem 1b that for high statistics $-2\\ln {\\cal L}$ is distributed like a $\\chi^2$ distribution. The uncertainty on the estimate of a parameter of the function can be obtained by finding how much the you can change the parameter to increase $-2\\ln {\\cal L}$ by 1.  In principle, because our likelihood has 2 parameters, the uncertainty on $\\tau$ and $\\kappa$ are correlated.  However, looking at your 2D plot from part 2c, you will see that the correlation between these parameters is in fact small.  We will ignore it here.  \n",
    "\n",
    "\n",
    "Using the value of $\\kappa$ obtained from the graph in part 2c, plot  $-2\\ln {\\cal L}$ for values of $\\Gamma$ around the maximum likelihood and indicate the uncertainty on the measured value of $\\tau$.  Do this for both the $N=1000$ and $N=10000$ event cases you studied in 2c."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5382b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1abf14a5",
   "metadata": {},
   "source": [
    "## Question 3:  Likelihood ratios and hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7b9509",
   "metadata": {},
   "source": [
    "One important use of statistics is to test hypothesws.  Suppose we want to determine whether our data is more consistent with hypothesis $H_0$ or hypothesis $H_1$, where the probability distribution functions are know for both hypotheses.   The Neyman-Pearson Lemma says that the region $W$ that minimizes the probability of wrongly accepting hypothesis $H_0$ is a contour in the likelihood ratio\n",
    "\n",
    "$$\n",
    "\\frac{{\\cal L}(x|H_1)}{{\\cal L}(x|H_0)} > \\kappa_\\alpha\n",
    "$$\n",
    "\n",
    "where $1-\\alpha$ is the probability for $H_0$ to be within the contour.  This Lemma and its proof are discussed in Kyle Crammer's 2020 Summer School Lecture https://indico.fnal.gov/event/43762/contributions/192672/attachments/132731/163512/HCPSS-stats-lectures-2020.pdf\n",
    "\n",
    "An example of the use of this likelihood ratio is an early LHC analysis from CMS that used the Higgs decay to 4 leptons to determine if the observed Higgs boson is indeed a scalar (Parity=+1)  or it is is instead a pseudoscalar (Parity=-1)\n",
    "\n",
    "\n",
    "<img src=\"cms-higgsParity.png\" alt=\"Drawing\" style=\"width:550px;\"/>\n",
    "\n",
    "\n",
    "For our purposes, this same idea can be used to determine whether the data are consistent with a background only hypothesis ($H_0$)  rather than a signal-plus-background hypothesis ($H_1$).  In this case, $H_0$ is often called the NULL hypothesis.\n",
    "\n",
    "We will explore this idea using the Gaussian signal plus flat background model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a98fe4e",
   "metadata": {},
   "source": [
    "### 3a."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb6f783",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Generate 1000 independent data samples each with 550 total events with a signal-to-background ratio of 0.1 (see Problem Set 3 for the code to do this and the definition of signal-to-background).  Also generate 1000 independent samples consisting of 550 events of pure background.  Use these samples to make a plot similar to the CMS plot shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79182aa0",
   "metadata": {},
   "source": [
    "### 3b. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca03d163",
   "metadata": {},
   "source": [
    "Repeat the experiment keeping the same signal-to-background ratio but increasing the number of events per data sample to 1000 and then 10000.  Describe what you see in these plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d251508a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa386513",
   "metadata": {},
   "source": [
    "### 3c."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42505a4",
   "metadata": {},
   "source": [
    "How many events would you need to collect so that for 99\\% of the time you could rule at the NULL hypothesis at 95\\% confidence level?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d71800",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622bfce7",
   "metadata": {},
   "source": [
    "### 3d."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45068a0f",
   "metadata": {},
   "source": [
    "Finally, generate a new set of data with a signal-to-background ratio of $R_{data} = 1$. Then define an array of rate values $\\vec{R}_{test} = \\{0.01, 0.1, 0.25, 0.5, 1, 2, 3\\} $. For each value of $R_{test}$, compute $-2\\log(L_1/ L_0)$ for 1000 experiments with 1000 events each. Plot the distribution of likelihood ratios for each $R_{test}$ versus the null hypothesis $R_{null} = 0$. Do this all on the same plot, and compare the results. Describe what happens as $R_{test} \\rightarrow R_{null}$, when $R_{test} < R_{data}$, when $R_{test} = R_{data}$, and when $R_{test} > R_{data}$. Does this behavior make sense? Explain your reasoning. \n",
    "\n",
    "NOTE: This might take a while to run. Be patient! If you need to write your code in a way that conserves memory, first generate the data for each value of $R_{test}$, then plot the histogram, then replace your old data with the new data with the next value of $R_{test}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f161c026",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bff0622",
   "metadata": {},
   "source": [
    "Write your explanations here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
